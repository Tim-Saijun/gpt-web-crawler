import requests
from bs4 import BeautifulSoup

def get_links(url):
    # å‘é€è¯·æ±‚å¹¶è·å–ç½‘é¡µå†…å®¹
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')
    url_split_list = url.split('/')
    base_url = url_split_list[0] + '//' + url_split_list[2]
    for i in range(len(url_split_list)-4):
        base_url = base_url + '/' + url_split_list[i+3]
    print(f"base_url:{base_url}")
    # æŸ¥æ‰¾æ‰€æœ‰çš„é“¾æ¥
    links = soup.find_all('a')
    unfiltered_links = [link.get('href') for link in links]
    print(unfiltered_links)
    relative_links = [url + link.get('href')[1:] for link in links if link.get('href').startswith('/')]
    # relative_links:['/lingdongyunjisuanshouye.html', '/shouye1.html', '/', '/jiaodianlingdongshouye.html']
    print(f"relative_links:{relative_links}")
    # è¿‡æ»¤å‡ºç¬¦åˆæ¡ä»¶çš„é“¾æ¥
    filtered_links = [link.get('href') for link in links if link.get('href').startswith(url)]
    # å°†filter_linksä¸relative_linksåˆå¹¶åˆ°ä¸€ä¸ªsetä¸­ï¼ŒåŒæ—¶æœ‰å»é‡çš„åŠŸèƒ½
    res_links = set(filtered_links + relative_links)
    return res_links

def recurrent_get_links(url, depth=1, max_count=100):
    links = get_links(url)
    if depth > 1 and len(links) < max_count:
        for link in links:
            recurrent_get_links(link, depth-1, max_count)
    return links

base_url = 'https://www.leadong.com'
start_path = '/'
start_url = base_url + start_path
# start_url = 'https://www.leadong.com/p/clients.html'
# è·å–åˆå§‹é“¾æ¥åˆ—è¡¨
page_links = get_links(start_url)
print(f"page_links:{page_links}, å…±è®¡é“¾æ¥æ•°é‡ğŸ”—:{len(page_links)}")
# mul_page_links = recurrent_get_links(start_url, depth=2, max_count=100)
# print(f"mul_page_links:{mul_page_links}, å…±è®¡é“¾æ¥æ•°é‡ğŸ”—:{len(mul_page_links)}")