import time
from turtle import st
import requests
from bs4 import BeautifulSoup
import json
import re
from .openai_extract import openai_extract
def replace_multiple_spaces_with_single(s):
    return re.sub(r'\s+', ' ', s)

def check(url):
    try:
        base_domain = url.split('/')[2]
    except:
        return None
    if url.startswith('https://'+base_domain) or url.startswith('http://'+base_domain):
        return url
    # if url.startswith('/'):
    #     return 'https://'+base_domain+url
    else:
        return None

def craw_single_page(url,use_template=False,use_ai=True):
    url = check(url)
    if not url:
        return [],[],0
    response = requests.get(url)
    if response.status_code == 200:
        # è§£æHTMLå†…å®¹
        soup = BeautifulSoup(response.text, 'html.parser')
        # print(soup.prettify())

        title = soup.title.string if soup.title else "N/A"
        # print(title)

        # æå–å…³é”®è¯
        keywords = soup.find('meta', attrs={'name': 'keywords'})
        if keywords:
            keywords = keywords['content']
        else:
            keywords = "N/A"

        description = soup.find('meta', attrs={'name': 'description'})
        if description:
            description = replace_multiple_spaces_with_single(description['content'])
        else:
            description = "N/A"

        # print("å…³é”®è¯ï¼š", keywords)
        # print("ç®€ä»‹ï¼š", description)

        # for content_text in soup.find_all('meta', content=True):
        #     print(content_text.get('content'))
        if use_template: # ä½¿ç”¨ç„¦ç‚¹å»ºç«™çš„æ¨¡æ¿
            body_area = soup.find('div', attrs={'id': 'backstage-bodyArea'})
            body_content = body_area.get_text() if body_area else "N/A"
            body_content = replace_multiple_spaces_with_single(body_content)
            # print(body_content)
        else: # ä¸€èˆ¬çš„ç½‘ç«™
            body_content = soup.body.get_text() if soup.body else "N/A"
            body_content = replace_multiple_spaces_with_single(body_content)
            # print(body_content)
        # ä½¿ç”¨AIå¤„ç†
        token_usage = 0
        meta_info = f"æ ‡é¢˜ï¼š{title}ï¼Œå…³é”®è¯ï¼š{keywords}ï¼Œç®€ä»‹ï¼š{description}"
        ai_extract_content,t = openai_extract(meta_info,body_content)
        token_usage += t
         # åˆ¤æ–­jsonæ˜¯å¦è§„èŒƒ
        status = True
        time_out = 0
        while status and time_out<5:
            try:
                json.dumps(ai_extract_content)
                status = False
            except:
                ai_extract_content,t = openai_extract(meta_info,body_content)
                token_usage += t
                time_out += 1
        
            
        print(ai_extract_content)
        print("ğŸ"*50)
        # ç»“ç®—
        page_res = {"title": title,"url":url, "keywords": keywords, "description": description, "body": body_content,"ai_extract_content":ai_extract_content}
        json_str = json.dumps(page_res, ensure_ascii=False)
        return page_res,json_str,token_usage

    else:
        print(url+"è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç ï¼š", response.status_code)
        return {},{},0


if __name__ == '__main__':
    url = "https://www.jiecang.cn/220513140440.html"
    res = craw_single_page(url)[0]
    print(res)


