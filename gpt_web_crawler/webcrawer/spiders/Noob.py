"""NoobSpiderçˆ¬å–ç½‘ç«™çš„åŸºæœ¬ä¿¡æ¯ï¼ŒåŒ…æ‹¬:
    - title
    - url
    - keywords
    - description
    - body
"""

from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import CrawlSpider, Rule
from ..items import NoobItem  # Import your Scrapy item here
import re
from bs4 import BeautifulSoup
import logging
import colorlog

# Log é…ç½®
fmt = "{asctime} {log_color}{levelname} {name}: {message}"
colorlog.basicConfig(style="{", format=fmt, stream=None)
# logging.basicConfig(level=logging.DEBUG, format="%(asctime)s %(levelname)s %(message)s",stream=None)
log = logging.getLogger(name="MySpider")
# log.setLevel(logging.DEBUG)


def replace_multiple_spaces_with_single(s):
    return re.sub(r'\s+', ' ', s)

class NoobSpider(CrawlSpider):
    name = 'noob'
    start_urls = []
    allowed_domains = []
    rules = ()
    def __init__(self, start_urls=None,
                     extract_rules=None,
                     *args, **kwargs):
            """
            Initialize the NoobSpider class.

            Args:
                start_urls (str): Comma-separated list of start URLs.
                extract_rules (str): Comma-separated list of extract rules.

            Raises:
                ValueError: If no start_urls or extract_rules are provided.
            """
            super(NoobSpider, self).__init__(*args, **kwargs)
            if start_urls:
                self.start_urls = start_urls.split(',')
            else:
                log.error("No start_urls provided!")
                raise ValueError("No start_urls provided!")
            
            self.allowed_domains = self.start_urls[0].split('/')[2]
            self.allowed_domains = [self.allowed_domains]
            
            if extract_rules:
                self.extract_rules_list = extract_rules.split(',')
            else:
                log.error("No extract_rules provided!")
                raise ValueError("No extract_rules provided!")
            self.rules = (
                # Rule(LinkExtractor(deny=[r'javascript:;'] + [r'/pd.*'] ), follow=True),
                # Rule(LinkExtractor(allow= self.extract_rules_list ), callback="parse_item",follow=True),
                        # è·Ÿè¸ªæ‰€æœ‰é“¾æ¥ï¼Œç›´åˆ°æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„é“¾æ¥
                # Rule(LinkExtractor(deny=[r'javascript:;',r'/pd.*']), follow=True),
                # # å¤„ç†ç¬¦åˆç‰¹å®šæ¨¡å¼çš„é“¾æ¥
                # Rule(LinkExtractor(allow=r'/pd.*'), callback="parse_item",follow=True),
                Rule(LinkExtractor(allow=self.extract_rules_list), callback="parse_item",follow=True),
                Rule(LinkExtractor(), follow=True),
            )
            self._compile_rules() 
            log.info("start_urls: %s", self.start_urls)
            log.info("extract_rules: %s", self.extract_rules_list)
            log.info("allowed_domains: %s", self.allowed_domains)
    """
    allowed_domains = ['www.jiecang.cn']
    start_urls = ['https://www.jiecang.cn/']
    rules = (
        # åœ¨Scrapyä¸­ï¼ŒRuleå¯¹è±¡åœ¨å¤„ç†æ—¶æœ‰ä¸€ä¸ªé¡ºåºã€‚å½“çˆ¬è™«é‡åˆ°ä¸€ä¸ªé¡µé¢å¹¶æå–å‡ºé“¾æ¥æ—¶ï¼Œå®ƒä¼šæŒ‰ç…§ruleså…ƒç»„ä¸­å®šä¹‰çš„é¡ºåºæ¥ä¾æ¬¡æ£€æŸ¥æ¯ä¸ªè§„åˆ™ã€‚ä¸€æ—¦æ‰¾åˆ°ä¸€ä¸ªåŒ¹é…çš„è§„åˆ™ï¼Œå°±ä¼šä½¿ç”¨è¯¥è§„åˆ™ï¼Œå¹¶ä¸å†ç»§ç»­æ£€æŸ¥åç»­çš„è§„åˆ™ã€‚æ›´å…·ä½“æˆ–æ›´é«˜ä¼˜å…ˆçº§çš„è§„åˆ™åº”è¯¥æ”¾åœ¨åˆ—è¡¨çš„å‰é¢ã€‚ğŸ˜­
        # è·Ÿè¸ªæ‰€æœ‰é“¾æ¥ï¼Œç›´åˆ°æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„é“¾æ¥
        Rule(LinkExtractor(deny=[r'javascript:;',r'/pd.*']), follow=True),
        # å¤„ç†ç¬¦åˆç‰¹å®šæ¨¡å¼çš„é“¾æ¥
        Rule(LinkExtractor(allow=r'/pd.*'), callback="parse_item",follow=True),
    )
    """
    def parse_item(self, response):
        item = NoobItem()

        item['title'] = response.xpath('//title/text()').get() or "N/A"
        item['url'] = response.url

        keywords = response.xpath('//meta[@name="keywords"]/@content').get()
        item['keywords'] = keywords if keywords else "N/A"

        description = response.xpath('//meta[@name="description"]/@content').get()
        item['description'] = replace_multiple_spaces_with_single(description) if description else "N/A"

        soup = BeautifulSoup(response.text, 'html.parser') # ä½¿ç”¨BeautifulSoupè§£æå‡ºå…¨éƒ¨å¯è¯»æ–‡æœ¬,Scrapyä¼¼ä¹åšä¸åˆ°
        body_text = soup.body.get_text(separator=' ', strip=True) if soup.body else "N/A"
        body_content = body_text
        item['body'] = body_content if body_content else "N/A"
        # log.info("body_content: %s", body_content)
        yield item
        
